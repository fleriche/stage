	Une fois tous les cas de tests rédigés, j'ai procédé à la réalisation d'une campagne de tests complète aboutissant à la génération d'un rapport. Afin de procéder à cela, pour chacun des tests j'ai utilisé l'outil \textit{Postman} qui est une plateforme proposant une interface graphique facilitant la construction de requêtes. Cet outil est conçu pour faciliter le développement des APIs en permettant de les intéroger de manière très rapide et simple sans avoir à développer un client pour les consommer. Dans l'optique de mettre à profit les tests effectués, nous avons connecté TestLink à Mantis, une application web permettant d'assurer le suivi des anomalies dans laquelle il est possible d'ouvrir des tickets concernant des bugs qui seront alors pris en charge par les développeurs jusqu'à leur clôture. Ainsi, lorsqu'un test échouait il était possible, d'un simple clique sur TestLink, de générer automatiquement un ticket sur Mantis avec toutes les informations rajoutées précédemment. Les développeurs avaient donc toutes les données (url, service etc...) pour reproduire l'anomalie en locale et la corriger. \\
	
	TestLink permet de personnaliser la génération des rapports de tests après l'exécution d'un plan complet. De plus, il est possible de générer le rapport dans différents formats. Nous avons donc décidé que chaque livraison comporterait un rapport complet contenant toutes les informations au format PDF. Cependant celui-ci pouvant être très conséquent, nous avons choisi de l'accompagner d'un rapport léger généré au format excel ne contenant que le nom des tests et leur statut (succès, echec, bloqué) avec un code couleur permettant a PBI de rapidement repérer les tests en échec et leur nombre. Ces derniers ont déclaré être très satisfait des nouveaux rapports fournis, c'est pourquoi il a été décidé que l'exécution des tests fonctionnels sur la plateforme TestLink serrait obligatoire avant chaque livraison. En attendant l'attribution d'un serveur pour héberger le gestionnaire de test, celui-ci a été mis en place sur le serveur personnel d'un des architecte du projet afin de le rendre accessible à toute l'équipe. \\
	
	La réalisation de ces tests et leur exécution m'a permis de relever certains écarts entre le produit réalisé et les spécifications. Par exemple, le service LoanDetails ne fournissait pas, dans sa réponse, certains champs demandé par le client. C'est pourquoi, après avoir centralisé tous les écarts que j'avais relevé, j'ai organisé une réunion avec mon chef de projet ainsi que certains développeurs dans le but de déterminer lesquels pourraient être corriger et lesquels ne le pourraient pas, nécessitant de prendre contact avec le client afin de clarifier la situation. Au terme de cette réunion, j'ai pu créer une nouvelle version des spécifications afin de les mettre à jour puis j'ai fait part de ces modifications à PBI à travers des mails rédigés en anglais. Ensuite, la mise en place de ces outils a permi à l'équipe de pouvoir effectuer les livraisons dans de meilleurs conditions puisque les anomalies pouvaient être détectées avant d'attendre les retours du client. De plus, tous les tests étaient centralisés, organisés et pouvaient être exécutés en parallèle par différentes personnes ce qui permettait un gain de temps à la fois sur l'exécution des tests mais aussi sur leur gestion (archivage, formalisme, perte de documents etc...). \\
	
	Néanmoins, afin d'exécuter les tests, les développeurs utilsaient Postman pour construire et envoyer les requêtes. Si cet outil est extrèmement utile pour développer un nouveau service ou tester un nouveau endpoint lors du développement, il n'est pas conçu pour exécuter un grand nombre de requête les unes après les autres à des fins de tests. En effet, comme nous l'avons expliqué dans la partie \ref{axway} les requêtes doivent posséder un token d'authentification dans leur header pour espérer passer la gateway et atteindre la couche microservices. Or, pour cela, il est nécessaire d'envoyer, toujours avec Postman, une requête de génération de ce token à l'API gateway. Après cela, il faut se connecter sur l'interface web fourni par Axway, retrouver la requête ainsi que sa réponse et copier le token. Cependant, pour accéder à cette interface il faut d'abord être en mesure d'accéder à l'environnement testé, Homo3 ou RGB, qui, rappelons le, n'ont pas les mêmes instances de l'API Gateway et ne sont pas accessibles de l'extérieur. Pour cela, il faut se connecter en RDP sur TSE puis ensuite utiliser les bonnes adresses et identifiants pour accéder à l'interface désirée. De retour sur Postman, il faut coller le token dans le header de la requête que l'on souhaitais tester. Cette démarche est illustrée sur l'annexe \ref{a3} qui décrit la procédure d'authentification gérée par la gateway. Et il faut répéter cette démarche \textbf{à chaque fois que le token expire}, ce qui ne pose pas de problèmes lorsque l'on souhaite envoyer une requête pour tester son code mais devient rapidement très fastidieux lorsque l'on a des centaines de requêtes à envoyer pour exécuter tous les tests. 
	Un exemple classique serait de tester le service LoanDetails. Pour cela il faut générer un token pour un abonné. Ensuite, on appelle le service ClientList pour afficher les produits de l'abonné puis on regarde si celui-ci possède un produit de type \textit{loan} pour récupérer son id (crédit en français). Enfin, on appelle le service LoanDetails avec l'id du loan obtenu précédemment. Si le client ne possède pas de loan il faut recommencer le processus avec divers abonnés jusqu'à trouver ce que l'on cherche. Maintenant, certains services nécessitent d'en   appeler trois autres différents ce qui implique de faire de nombreuses combinaisons avant de tomber sur celle qui nous permet de réaliser le test. Il résulte de cela une perte considérable de temps qui aurrait pu être mis au profit du développement, de l'amélioration des points jugés sensibles ou encore de la correction des anomalies. En effet, il n'était pas rare que l'ensemble des tests puissent occuper une personne presque \textbf{une demi journée}, ce qui se révèle être énorme sur des sprints de deux ou trois semaines. Il fallait donc trouver le moyen de conserver la procédure de test et la génération de rapports tout en réduisant drastiquement le temps que cela demandait, c'est pourquoi nous avons décidé d'automatiser les tests fonctionnels.
	
	